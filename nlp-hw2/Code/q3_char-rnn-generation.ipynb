{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions:\n",
    "1. **Restart the kernel** (in the menubar, select Runtime$\\rightarrow$Restart runtime)\n",
    "2. **Run all cells** (in the menubar, select Runtime$\\rightarrow$Run All).\n",
    "3. **Download the notebook** (in the menubar, select File$\\rightarrow$Download .ipynb)\n",
    "4. **Add the downloaded notebook (.ipynb file) to the submission zip**.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"**WRITE YOUR ANSWER IN THIS CELL**\", and that no tests fail.  \n",
    "Write the IDs of all group members in the cell below. Leave any surplus IDs as `\"\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID1 = \"\"  \n",
    "ID2 = \"\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5493400e8b7f9a8e2cde874866d4fa7f",
     "grade": false,
     "grade_id": "cell-3a1bca1dbb7d0069",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![shakespeare](https://i.imgur.com/81YZuel.jpg)\n",
    "\n",
    "# Generating Shakespeare Using a Character-level Language Model\n",
    "\n",
    "### From Words to Characters\n",
    "In the previous two sections we dealt with word-level language models. But looking again at section 2, there is nothing that constraints us to using _words_ as the basic elemnents in our model. The model we analyzed in section 2 could just as well be character-based - just replace \"word\" with \"character\", and you are good to go. In this notebook we will train a small character-based language model that will help us generate Shakespearean-like (emphasis on the _like_...) texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9af7a343d0e3524c3fd846d987d766a8",
     "grade": false,
     "grade_id": "cell-7301754e4d655d01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 3.a\n",
    "Can you think of an advantage a character-based language model could have over a word-based language model? _(You might find question 2.c useful)_. And what about the other way around: can you think of an advantage a word-based language model could have over a character-based language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb742db028fffc9c69d14202fc1e24bd",
     "grade": true,
     "grade_id": "cell-e19646c939692ee9",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**WRITE YOUR ANSWER IN THIS CELL**\n",
    "We think that a character-based language model can be better in languages like Hebrew or German, where each word is a construction of few sub-words. For example the Hebrew word (ובבתיהם) contains 3 meaning units: “in” “their” and “houses”. It is very reasonable to assume that this word won’t appear much at this form, but rather in a few other forms of the word “בית”. Therefore, it will be hard to train word- based model (sparsity), as opposed to char based model.\n",
    "On the other hand, because in a text there are significantly less words then chars, words based model will have less layers and will work faster. Additionally, it will grasp the semantic better then char-based model, because he will need less layers to connect 2 different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d46a8dfd340b8f68e51a041307f7d7d3",
     "grade": false,
     "grade_id": "cell-ebc0d8ae3061c0fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Using PyTorch\n",
    "\n",
    "We'll build our language model using PyTorch. PyTorch is a [very popular](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/) open-source machine learning (and deep learning) framework developed by Facebook. In short:\n",
    "\n",
    "> Pytorch is a Python-based scientific computing package targeted at two sets of audiences:\n",
    "* A replacement for NumPy to use the power of GPUs\n",
    "* A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "To get familiar with PyTorch, check out this [quick tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html). In addition, another imporant difference from numpy is that PyTorch can automatically calculate the gradients needed for backpropagation, as explained [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02af8a21a2e8fae58d84f915de5b016d",
     "grade": false,
     "grade_id": "cell-aa2773db1bef7014",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Our dataset is a plain text file. For simplicity, we turn any potential unicode characters into plain ASCII by using the `unidecode` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef0359e8c08b2057771c115150011e7e",
     "grade": false,
     "grade_id": "cell-cce75419c097f3fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in our dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "import unidecode\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)  # our vocabulary size (|V| from the handout)\n",
    "\n",
    "dataset_as_string = unidecode.unidecode(open('data/shakespeare.txt').read())\n",
    "n_chars_in_dataset = len(dataset_as_string)\n",
    "print(f'Total number of characters in our dataset: {n_chars_in_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06dd2ac91a6296206475c7e330e53e3d",
     "grade": false,
     "grade_id": "cell-d795f907dd7922f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To make inputs out of this big string of text, we will split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61947ad22fb7f16eba246d47ab8cae22",
     "grade": false,
     "grade_id": "cell-379f229536dae19b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:\n",
      "And after many scorns, many foul taunts,\n",
      "They took his head, and on the gates of York\n",
      "They set the same; and there it doth remain,\n",
      "The saddest spectacle that e'er I view'd.\n",
      "\n",
      "EDWARD:\n",
      "Sweet Duke of York, our prop to lean upon,\n",
      "Now thou art gone, we have no staff, no stay.\n",
      "O Clifford, boisterous Clifford! thou hast slain\n",
      "The flower of Europe for his chivalry;\n",
      "And treacherously hast thou vanquish'd\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 400\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, n_chars_in_dataset - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return dataset_as_string[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba5d4900ff254fa335fe935962878c8d",
     "grade": false,
     "grade_id": "cell-fcbb2d73f4e442fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Building Our Model\n",
    "\n",
    "Our model consists of three main components:\n",
    "\n",
    "1. [**Embedding**](https://pytorch.org/docs/stable/nn.html#embedding). A mapping between characters and their learned representations (\"word vectors\") \\[correspoding to ${\\boldsymbol L}$ in terms of the handout\\]\n",
    "2. [**GRU**](https://pytorch.org/docs/stable/nn.html#gru). \\[correspoding to the computation of ${\\boldsymbol h}^{(t)}$ in terms of the handout\\]\n",
    "3. **Output Layer**. A feed-forward neural network that transforms a hidden state at a timestep into a probability distribution of the next character. \\[correspoding to the computation of $\\hat{\\boldsymbol y}^{(t)}$ in terms of the handout\\] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.b\n",
    "Complete the implementation of the `forward` method of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9ad1239fcd5aec23f439249397895ec",
     "grade": false,
     "grade_id": "cell-1640492438386e87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class OurModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(OurModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # In the terms of the handout, here d = D_h\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_, hidden):\n",
    "        # General instructions:\n",
    "        # Pass the embedded input through the GRU and use the output layer to get the next character distribution.\n",
    "        # return that distribution and the next hidden state.\n",
    "        # You may need to play around with the dimensions a bit until you get it right. Dimension-induced frustration is good for you!\n",
    "        # -------------------------\n",
    "        # YOUR CODE HERE\n",
    "        input           = self.embedding(input_.view(1, -1))\n",
    "        output, hidden  = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output          = self.output_layer(output.view(1, -1))\n",
    "        # -------------------------\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.num_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da793a49917dc4882e7e70f04d07a777",
     "grade": false,
     "grade_id": "cell-b9299fddeb082b4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Creating the Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6eaeb80c370b32f26eda2ac1be57444",
     "grade": false,
     "grade_id": "cell-83bf9e1b0374206c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Each chunk will be turned into a tensor by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc87bca342db2fde1b3957f48bcfe857",
     "grade": false,
     "grade_id": "cell-5360afdd0b03b1f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# Turn a string into list of longs\n",
    "def chars_to_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(chars_to_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32c02aa64cee9046c2917487ac982de0",
     "grade": false,
     "grade_id": "cell-6e7b3d9e8c9396bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Each training example for our model will be created from a chunk randomly extracted from our shakespeare dataset. For example, if we set our chunk size to be 28, then a randomly extracted chunk could be $\\texttt{As deep as that, though true}$. Each training example is of a form $(\\textbf{x},\\textbf{y})$ where $\\textbf{x}$ is all the charecters of the chunk *except the last* and $\\textbf{y}$ is all the charecters of the chunk *except the first*. For example, given the chunk above, $\\textbf{x}=\\texttt{As deep as that, though tru}$ and $\\textbf{y}=\\texttt{s deep as that, though true}$. At timestep i our input is $\\textbf{x}^{(i)}$ and the gold label our model will try to predict is $\\textbf{y}^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f159d648c0ab9ffbdc096aeac6905a57",
     "grade": false,
     "grade_id": "cell-d3539c5f1d96a188",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def random_training_example():    \n",
    "    chunk = random_chunk()\n",
    "    inp = chars_to_tensor(chunk[:-1])\n",
    "    target = chars_to_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18a6bf800d9bc590739b15ba01dda408",
     "grade": false,
     "grade_id": "cell-16d13f3b273395ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fffa10554299eaae14cc007fea3935a",
     "grade": false,
     "grade_id": "cell-1d3fd015fe8f64d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a47c721a818979b886f119401206e756",
     "grade": false,
     "grade_id": "cell-44ab27a8fee696ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = model.init_hidden()\n",
    "    prime_input = chars_to_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist =  F.softmax(output / temperature, dim=-1)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = chars_to_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a98218b35f47137eeba1ba1aead0700",
     "grade": false,
     "grade_id": "cell-a209b293a8850a57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb3bfcd4d49b2f2672447d8c65b6cb05",
     "grade": false,
     "grade_id": "cell-e246cbd9689e1a6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = model(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfb863e279db4b170c35d8d0c7a37a1f",
     "grade": false,
     "grade_id": "cell-05ce9b9275e0d1cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16d7b53f211a6a1bef71c1dd2d1271cf",
     "grade": false,
     "grade_id": "cell-cb78afef7022f9a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{m}m {math.floor(s)}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b368f1767ddd0eddca44249fa47ed32",
     "grade": true,
     "grade_id": "cell-98f46bec0b8c87cc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98abd7dd7805753c2e7b635f1265cb73",
     "grade": false,
     "grade_id": "cell-baf25642209867dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7392249521c153ef4d7713eecb8204a",
     "grade": false,
     "grade_id": "cell-4900f92ae503be69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time elapsed: 0m 25s  ;  iterations: 100 (5.0%)  ;  loss: 2.175]\n",
      "Whe you thel an the wil an wor at in im and I you ial thit an walll me gay my o she il dray burich lie th nor stin fared ler you frer, your mane preat s maray und my th weroke as soung ou pase gen porvi \n",
      "\n",
      "[time elapsed: 0m 53s  ;  iterations: 200 (10.0%)  ;  loss: 2.05]\n",
      "Wher, dour prownd enf[ingiled row\n",
      "To wor let fin dowdge to thof fith were agales?\n",
      "\n",
      "Seclenging I's hernare come ming mans, I wo gromed,\n",
      "Mor Ky coof heres and dom eand lut kith in of tow ther?\n",
      "\n",
      "Mand orngi \n",
      "\n",
      "[time elapsed: 1m 20s  ;  iterations: 300 (15.0%)  ;  loss: 1.942]\n",
      "Whos is cears then gavelves.\n",
      "\n",
      "LOCUTE:\n",
      "We hou thoussed, in beppore endangany.\n",
      "\n",
      "FRCIO:\n",
      "Hom be low,\n",
      "Belbare to by am and nald beced the ben is pretret heave oke.\n",
      "\n",
      "AULE:\n",
      "Aclend in my ablles Iald is amanely  \n",
      "\n",
      "[time elapsed: 1m 47s  ;  iterations: 400 (20.0%)  ;  loss: 1.791]\n",
      "What my have your the kised I not that, the I sherve,\n",
      "That you sy weed if, coarfter, Sigsir, west cray his have a feer,\n",
      "Mavice, Greamn to he see kind are betrue not sird, then,\n",
      "I live see, be thy lords  \n",
      "\n",
      "[time elapsed: 2m 13s  ;  iterations: 500 (25.0%)  ;  loss: 1.818]\n",
      "Why asio?\n",
      "\n",
      "BANTA:\n",
      "Whis ming are his the sirn for that saiser my the gread.\n",
      "\n",
      "HENRY HESTTE:\n",
      "He fay why has mupt thint treat the vitenus to shall\n",
      "And I that afe masteler coum, do me not and of the with tha \n",
      "\n",
      "[time elapsed: 2m 40s  ;  iterations: 600 (30.0%)  ;  loss: 1.966]\n",
      "Whingner soH him be faricents\n",
      "The repelf's wouw as soooW ill werenty,\n",
      "We is soly, Yor it; and grom dearved heaver he cours\n",
      "Theer an fold not reath the sele quover in that the read consiltner and his.\n",
      "\n",
      "F \n",
      "\n",
      "[time elapsed: 3m 8s  ;  iterations: 700 (35.0%)  ;  loss: 1.746]\n",
      "Wh I as child at but marrect\n",
      "Whee and furm the posure on of oo the is and meb,\n",
      "Whunder my wo, the a bake une,\n",
      "And, allaff I dones the coums.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Self and the may his deself me:\n",
      "That the sor \n",
      "\n",
      "[time elapsed: 3m 35s  ;  iterations: 800 (40.0%)  ;  loss: 1.769]\n",
      "What bound, the have muthed the plody\n",
      "And you death.\n",
      "\n",
      "SIANDIO:\n",
      "Seat will aferby, lake your good prombline of geour their recaunest the hims;\n",
      "And dook my beat hable leevice challn not the recelings\n",
      "Or wh \n",
      "\n",
      "[time elapsed: 4m 2s  ;  iterations: 900 (45.0%)  ;  loss: 1.973]\n",
      "What the fill, the cainf their The with for sweet,\n",
      "As for were am fair must to death old to own the gold, the cure comess;\n",
      "And bother make sir man to wanded not thiswer ward,\n",
      "And yemple of coman yought  \n",
      "\n",
      "[time elapsed: 4m 30s  ;  iterations: 1000 (50.0%)  ;  loss: 1.804]\n",
      "Whom thought is brother:\n",
      "Margio, lild his sweet be praidection bet\n",
      "Whis in agaest! All.\n",
      "\n",
      "GULIUS:\n",
      "What thy swork thought, not have garty\n",
      "In that mean not to batter's gloods.\n",
      "\n",
      "GLOUCESTER:\n",
      "O' tis at ear yo \n",
      "\n",
      "[time elapsed: 4m 57s  ;  iterations: 1100 (55.00000000000001%)  ;  loss: 2.0]\n",
      "Whib die\n",
      "Well herself, a more ito theys.\n",
      "\n",
      "KING EDWARD III:\n",
      "Gedseath Ruchood and and to they of manness\n",
      "To have fall. What I when cures,\n",
      "I wome, the are aing them know,\n",
      "That it it of best, will not the f \n",
      "\n",
      "[time elapsed: 5m 25s  ;  iterations: 1200 (60.0%)  ;  loss: 1.777]\n",
      "Whery when churse fearl I have in here.\n",
      "\n",
      "NUMIONUS:\n",
      "A, it pringate stall, of was first of your juing glard forger,\n",
      "That a what bausel it we ere and remich recin,\n",
      "And ele my fill a would be mant prace;\n",
      "An \n",
      "\n",
      "[time elapsed: 5m 52s  ;  iterations: 1300 (65.0%)  ;  loss: 1.695]\n",
      "Whing eyop stander the keen old death the\n",
      "comper and eewy lades com their it proms.\n",
      "But morage, swave, and and eye tell men, the love\n",
      "Than to lest tip man, this hears?\n",
      "\n",
      "Fersive to this of\n",
      "For may the ha \n",
      "\n",
      "[time elapsed: 6m 19s  ;  iterations: 1400 (70.0%)  ;  loss: 1.809]\n",
      "Whand I dever, as have my knot here all now\n",
      "recortist God do hath the clomes the forsture,\n",
      "What to encord not a deads,\n",
      "For this fungear that which and then you hortied sounnel.\n",
      "Ageted? I lovo my cerpour \n",
      "\n",
      "[time elapsed: 6m 47s  ;  iterations: 1500 (75.0%)  ;  loss: 1.854]\n",
      "Wh thee time be in thee,\n",
      "For is that such and but be mane and bround you sir,\n",
      "for eyes hance word counner to greand since and on\n",
      "For and and no axpir old is it intence\n",
      "ant king.\n",
      "\n",
      "COMINIUS:\n",
      "We have be in \n",
      "\n",
      "[time elapsed: 7m 14s  ;  iterations: 1600 (80.0%)  ;  loss: 1.805]\n",
      "What lain the peath ditts,\n",
      "nand the were them plaiged leattle and precion,\n",
      "I have set; think of the wile fir;\n",
      "This fame listerlathing sore not the king,\n",
      "ANow, would right, let first, be setrious\n",
      "Think b \n",
      "\n",
      "[time elapsed: 7m 41s  ;  iterations: 1700 (85.0%)  ;  loss: 1.764]\n",
      "Whand elious of your in\n",
      "And my hon lies the heart's are and bebell you from with in you,\n",
      "For my lips parrint allow more wishas say.\n",
      "\n",
      "POLIXENES:\n",
      "Mark, sir, mother wife the is, be the plicklice you,\n",
      "Sich  \n",
      "\n",
      "[time elapsed: 8m 9s  ;  iterations: 1800 (90.0%)  ;  loss: 1.649]\n",
      "What is in my lond in parl?\n",
      "In as at upon of mistlem,\n",
      "Begring elsed is the not it my lord.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "No kind sing would and bound the displess,\n",
      "Once and the hall and was groun, for that begrends \n",
      "\n",
      "[time elapsed: 8m 37s  ;  iterations: 1900 (95.0%)  ;  loss: 1.664]\n",
      "Which and pends father in them thee,\n",
      "But I lied and the when what and periend,\n",
      "Finds speak to cheisest his your deed you\n",
      "thy lord.\n",
      "\n",
      "GREMEO:\n",
      "The let the fear. That of the in the charf,\n",
      "Do no the so for s \n",
      "\n",
      "[time elapsed: 9m 5s  ;  iterations: 2000 (100.0%)  ;  loss: 1.524]\n",
      "Wher the hence thought yetniets\n",
      "Upon that at the fearp, on so cono,\n",
      "The have breas four list all his souls:\n",
      "The lifine done the despied hornguest his power are thou sclate\n",
      "Where less his give a faors, w \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100  # (D_h from the handout)\n",
    "num_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "model = OurModel(n_characters, hidden_size, n_characters, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    loss = train(*random_training_example())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if iteration % print_every == 0:\n",
    "        print(f'[time elapsed: {time_since(start)}  ;  iterations: {iteration} ({iteration / n_iterations * 100}%)  ;  loss: {loss:.4}]')\n",
    "        print(evaluate('Wh', 200), '\\n')  # generate text starting with 'Wh'\n",
    "\n",
    "    if iteration % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8584d3be75d90a5197e7133411e0021d",
     "grade": false,
     "grade_id": "cell-ff9d72dafefa0a23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training Loss\n",
    "\n",
    "Plotting the the losses that were computed during training can provide a further indication that the network was indeed learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77791204e14b67d3ad68d70695c479a6",
     "grade": false,
     "grade_id": "cell-f91bb597844b8f7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ccff0a6388>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('# of iterations (divided by plot_every)')\n",
    "plt.ylabel('average loss')\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a376c5ed8b8f2dff9ea4731669bc8a49",
     "grade": false,
     "grade_id": "cell-dac1f386e2b526f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluating at Different Temperatures\n",
    "Every time we use the `evaluate` function to generate the distribution of the next character, we don't just use softmax as usual, but we also divide by a `temperature`.  \n",
    "Let's examine the effect of changing the temperature when generating text using our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That the so so so sorrow.\n",
      "\n",
      "MARIANA:\n",
      "The so so so so so so so so so so soul'd the for the so some\n",
      "The so so so so so his death her for the so so so soul'd\n",
      "The so so so so so so so so so soul'd the so so so some\n",
      "The so so so so so his death the for the so so so soul'd\n",
      "The death the so so so so so so so so so some the some\n",
      "The so his so so so so so so so so so so so so some\n",
      "The so so so so so so so so \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 400, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That the say this like your bewone\n",
      "The did compants the for the peers her not honess,\n",
      "Thou have his celest that stroke in the serve and the so the sten vichless\n",
      "This beace a couse at that with this so lead of the too master of a some\n",
      "The so Clipter in my love the for your sore in\n",
      "Her, dainest of my from he gried in that the deen from\n",
      "That love see the pealter a presser for the fair falest of thy\n",
      "sho\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 400, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That then with lay my for say\n",
      "Ay, the for life for he ctarry the ere my boot,\n",
      "Than at done gorsing thee know to morers bent-to his?\n",
      "\n",
      "CAPULET:\n",
      "Who the son'd myself so haster when at it say.\n",
      "\n",
      "PRANIO:\n",
      "The king Mantreed will bettering broth;\n",
      "Night gave thinst other priest stranio I love love you know and my love soul'd, I\n",
      "\n",
      "HENA:\n",
      "What I fear as brother and as with it the pleforitor'd.\n",
      "Shall a common: boy\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 400, temperature=0.8))  # the default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That And as your sight\n",
      "Talicies love Is it shall villess friin his filled all.\n",
      "\n",
      "HENGRIAS:\n",
      "He's fet at verve of than you desoul gives.\n",
      "\n",
      "THOMPRONENI:\n",
      "My have is tish'd, angir--whis now he mine is else.\n",
      "So his have more untow a grout-do so'll'd in\n",
      "the madge sayeld man themTy bore\n",
      "whisy to--twaule.\n",
      "\n",
      "HENRY ANNE:\n",
      "Why your grait; faith nellow tonguest then?\n",
      "\n",
      "COMINIUS:\n",
      "For you batherd it in was to returpst \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 400, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thilve a to ningetion'\n",
      "Weatherean thif her o, andy inteer!\n",
      "A ei, Ae-gritosed Itlego dry dnart;\n",
      "for crobficative, Rome, falliag my ma?\n",
      "Good'd,\n",
      "For he so Cumlar waor meptisabours to,ture: dOut'ty?\n",
      "The patherAh y laindd'll enomps thee; yo, undirs,\n",
      "Firle? O aploye. Of hopporm.\n",
      "\n",
      "RILA AI:\n",
      "I'll Dres: wisof?' wondrenkoflocsits,, foave with uft washil\n",
      "grow.\n",
      "By keen seit.\n",
      "\n",
      "YORK:\n",
      "Mo'th have eviced?\n",
      "\n",
      "thinwerut,\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 400, temperature=1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33ad994fe23b1e288fc8ef61422435cd",
     "grade": false,
     "grade_id": "cell-08d4f5f563b097a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 3.c\n",
    "How does the value of `temperature` affect the properties of the generated text?\n",
    "Specifically address the process of sampling a character from the next character distribution, and the effect `temperature` has on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73f841dadc6afd8d63765a083c3f8914",
     "grade": true,
     "grade_id": "cell-bfdb7d9e747067db",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**WRITE YOUR ANSWER IN THIS CELL**\n",
    "Temperature affects the softmax output;\n",
    "High temperature will result with more evenly distributed probabilities for the tokens whereas lower temperature will result with less evenly distributed probability and more towards the tokens with the original higher softmax probabilities. \n",
    "Higher temperature yields more diverse and random text, with more mistakes.\n",
    "Lower temperature yields more predictable and repetitive text with same high probability tokens appearing more. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
