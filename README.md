![visitors](https://visitor-badge.glitch.me/badge?page_id=AvivYaniv.Natural-Language-Processing.issue.1) <br/>
[![HitCount](http://hits.dwyl.com/AvivYaniv/Natural-Language-Processing.svg)](http://hits.dwyl.com/AvivYaniv/Natural-Language-Processing) <br/>

# Natural Language Processing

Natural Language Processing (NLP) aims to develop methods for processing, analyzing and understanding natural language. The goal of this class is to provide a thorough overview of modern methods in the field of Natural Language Processing. 

<p align="center">
    <img src="https://github.com/AvivYaniv/Natural-Language-Processing/blob/master/logo/NLP.png" width="50%"/>
<p/>

## Assignments
[Assignment 1](https://github.com/AvivYaniv/Natural-Language-Processing/blob/master/nlp-hw1/hw1.pdf) : Word Vectors <br/>
- Count-Based Word Vectors <br/>
- Understanding Word2Vec <br/>
- Implementing [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) <br/>
- Exploring [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) Embeddings <br/>


[Assignment 2](https://github.com/AvivYaniv/Natural-Language-Processing/blob/master/nlp-hw2/nlp-hw2.pdf) : Language Models <br/>
- Word-Level Neural Bigram Language Model <br/>
- Theoretical Inquiry of a Simple RNN Language Model <br/>
- Generating Shakespeare Using a Character-level Language Model <br/>


[Assignment 3](https://github.com/AvivYaniv/Natural-Language-Processing/blob/master/nlp-hw3/nlp-hw3.pdf) : Tagging <br/>
- Data Preprocessing <br/>
- Most Frequent Tag Baseline <br/>
- [Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) & [HMM](https://en.wikipedia.org/wiki/Hidden_Markov_model) Tagger <br/>
- [Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) & [Maximum Entropy Markov Model (MEMM)](https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model) Tagger <br/>
- [BiLSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) Tagger <br/>


[Assignment 4](https://github.com/AvivYaniv/Natural-Language-Processing/blob/master/nlp-hw4/nlp-hw4.pdf) : Dependency Parsing <br/>
- Neural Transition-Based Dependency Parsing <br/>

[Assignment 5](https://github.com/AvivYaniv/Natural-Language-Processing/blob/master/nlp-hw5/nlp_hw5_sempar_seq2seq.ipynb) : Semantic Parsing <br/>
- Simple Seq2seq Model, implementing EncoderRNN & DecoderRNN <br/>
- Attention Seq2seq Model, implementing DecoderAttention <br/>

